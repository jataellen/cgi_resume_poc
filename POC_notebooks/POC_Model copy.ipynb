{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37e94a4c-506f-4d12-a012-06be8e265326",
   "metadata": {},
   "source": [
    "# POC Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4456a4e2-4227-42ee-83e9-71d5ff3234d8",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "04a17587-84d0-44db-83bc-399925a890c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import AzureOpenAI\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "import json\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import datetime\n",
    "from docx import Document\n",
    "from dotenv import load_dotenv\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8c1ca7-04ce-440e-af2d-b61de11dc793",
   "metadata": {},
   "source": [
    "### Inits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b1567e52-8f25-4d96-8342-397a89e02dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "63380eac-4ed4-4de9-8f65-c146f1e3db9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "    api_version=\"2024-12-01-preview\",\n",
    "    deployment_name=\"gpt-4o\",\n",
    "    model=\"gpt-4o\",  # Ensure function calling support\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09c386b-a79b-4358-8723-aa8fac3969c5",
   "metadata": {},
   "source": [
    "### Load PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "41f709c5-985b-48f0-8804-11413a1744c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load the JSON file\n",
    "file_path = \"../resume_data.json\" \n",
    "\n",
    "# Open and parse the JSON file\n",
    "with open(file_path, 'r') as file:\n",
    "    structured_data = json.load(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32568594-875c-47ce-b9fb-bef481e68208",
   "metadata": {},
   "source": [
    "### Structure file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4cb68f95-e777-44bf-a6f7-fb8fd85b89c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_date = datetime.datetime.now().date()\n",
    "\n",
    "with open('../data/json_schema.json', 'r') as file:\n",
    "    json_schema = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "751caa70-d2b4-4cc4-b21f-9b48b749dba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw LLM Response: ```json\n",
      "{\n",
      "  \"evaluation\": {\n",
      "    \"profile\": {\n",
      "      \"things_done_poorly\": [\n",
      "        \"The profile is overly dense, making it hard to quickly grasp key information at a glance.\",\n",
      "        \"Some details about soft skills or leadership style (e.g., communication, adaptability) are missing, despite leadership roles being highlighted.\",\n",
      "        \"It lacks specific measurable impacts of contributions, such as quantitative metrics to illustrate outcomes (e.g., percentage improvement, reduction in time, etc.).\"\n",
      "      ],\n",
      "      \"things_done_well\": [\n",
      "        \"The profile provides a clear overview of technical expertise and relevant work experience.\",\n",
      "        \"It effectively connects academic achievements with professional and research accomplishments.\",\n",
      "        \"Complex roles and projects are described in a way that highlights innovation and relevance to the NLP and machine learning fields.\"\n",
      "      ],\n",
      "      \"rating\": 8,\n",
      "      \"flag\": false\n",
      "    },\n",
      "    \"skills\": {\n",
      "      \"things_done_poorly\": [\n",
      "        \"The skills section is overly comprehensive, which may dilute focus on the most relevant skills for an intended role.\",\n",
      "        \"Skill levels and years of experience listed are inconsistent between different parts (e.g., structured data vs. skills summary).\",\n",
      "        \"Some niche skills (e.g., React and HTML/CSS) may not align with the overarching NLP and machine learning expertise, potentially creating confusion about focus.\"\n",
      "      ],\n",
      "      \"things_done_well\": [\n",
      "        \"The skills are diverse and encompass both technical expertise and industry-specific knowledge.\",\n",
      "        \"The breakdown of technical skills into specific domains (e.g., statistics, machine learning) is helpful for recruiters to quickly match capabilities to job requirements.\",\n",
      "        \"It emphasizes highly specialized and industry-relevant skills such as Prompt Engineering and NLP for Low Resource Languages.\"\n",
      "      ],\n",
      "      \"rating\": 7,\n",
      "      \"flag\": false\n",
      "    },\n",
      "    \"experience\": {\n",
      "      \"things_done_poorly\": [\n",
      "        \"Some dates are incomplete or unclear (e.g., '11/25' as a start date instead of providing the full year).\",\n",
      "        \"Quantifiable achievements are missing in most roles, leaving vague descriptions of responsibilities without direct impact metrics (e.g., time savings, revenue increase, performance improvements).\",\n",
      "        \"Certain responsibilities overlap across roles (e.g., Python scripting mentioned multiple times), limiting the uniqueness of contributions across experiences.\"\n",
      "      ],\n",
      "      \"things_done_well\": [\n",
      "        \"The experience section showcases diverse roles, including leadership and technical expertise in globally recognized companies like Amazon and Meta.\",\n",
      "        \"It highlights innovation and thought leadership, particularly in cutting-edge topics like AI safety, privacy compliance, and prompt engineering.\",\n",
      "        \"Each role description emphasizes collaboration across diverse teams, indicating strong teamwork and cross-functional communication skills.\"\n",
      "      ],\n",
      "      \"rating\": 7,\n",
      "      \"flag\": false\n",
      "    }\n",
      "  }\n",
      "}\n",
      "```\n",
      "Error decoding JSON: Expecting value: line 1 column 1 (char 0)\n",
      "Warning: Section 'Profile' is missing in the response.\n",
      "Warning: Section 'Skills' is missing in the response.\n",
      "Warning: Section 'Experience' is missing in the response.\n",
      "Ratings: {'Profile': 0, 'Skills': 0, 'Experience': 0}\n",
      "Overall Score Calculation: 0.0\n",
      "{\n",
      "  \"overall_score\": 0.0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "\tSystemMessage(\n",
    "\t\tcontent=\"You are an AI that evaluates resume sections and provides ratings and feedback.\"\n",
    "\t),\n",
    "\tHumanMessage(\n",
    "\t\tcontent=(\n",
    "\t\t\tf\"Using the following structured resume data in JSON format:\\n\\n{json.dumps(structured_data, indent=2)}\\n\\n\"\n",
    "\t\t\t\"For each section (Profile, Skills, Experience), return:\\n\"\n",
    "\t\t\t\"- 3 things done poorly.\\n\"\n",
    "\t\t\t\"- 3 things done well.\\n\"\n",
    "\t\t\t\"- A rating out of 10.\\n\"\n",
    "\t\t\t\"- Flag if the rating is under 6.\\n\\n\"\n",
    "\t\t\t\"Return the output as valid JSON.\"\n",
    "\t\t)\n",
    "\t),\n",
    "]\n",
    "\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "print(\"Raw LLM Response:\", response.content)\n",
    "\n",
    "\n",
    "if not response.content.strip():\n",
    "\tprint(\"Error: Response content is empty.\")\n",
    "\tevaluation_feedback = {}\n",
    "else:\n",
    "\t# Parse the response\n",
    "\ttry:\n",
    "\t\tevaluation_feedback = json.loads(response.content)\n",
    "\t\tprint(\"Parsed JSON:\", json.dumps(evaluation_feedback, indent=2))  # Debugging print\n",
    "\texcept json.JSONDecodeError as e:\n",
    "\t\tprint(\"Error decoding JSON:\", e)\n",
    "\t\tevaluation_feedback = {}\n",
    "\n",
    "# Define sections and weights\n",
    "sections = [\"Profile\", \"Skills\", \"Experience\"]\n",
    "weights = {\"Profile\": 0.4, \"Skills\": 0.3, \"Experience\": 0.3}\n",
    "\n",
    "\n",
    "ratings = {}\n",
    "for section in sections:\n",
    "\tif section in evaluation_feedback.get(\"evaluation\", {}):\n",
    "\t\tratings[section] = evaluation_feedback[\"evaluation\"][section].get(\"rating\", 0)\n",
    "\telse:\n",
    "\t\tprint(f\"Warning: Section '{section}' is missing in the response.\")\n",
    "\t\tratings[section] = 0  # Default to 0 for nowwww\n",
    "print(\"Ratings:\", ratings)  # Debugging print\n",
    "\n",
    "\n",
    "overall_score = sum(ratings[section] * weights[section] for section in sections)\n",
    "print(\"Overall Score Calculation:\", overall_score)\n",
    "\n",
    "# Add the overall score to the feedback\n",
    "evaluation_feedback[\"overall_score\"] = round(overall_score, 2)\n",
    "\n",
    "# Print the updated feedback\n",
    "print(json.dumps(evaluation_feedback, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b79f0a",
   "metadata": {},
   "source": [
    "### Experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d1c8f4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "unsep_json = {\"experience\": {\n",
    "    \"json_schema\": {\n",
    "        \"name\": \"formatted_experience\",\n",
    "        \"description\": \"Formats the experience section of a resume into structured JSON without separating CGI Experience and Other Experience.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"experience\": {\n",
    "                    \"type\": \"array\",\n",
    "                    \"description\": \"List of all job experiences.\",\n",
    "                    \"items\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"company\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Company name\"\n",
    "                            },\n",
    "                            \"sector\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Industry of the type of work (e.g., Healthcare, Financial services)\"\n",
    "                            },\n",
    "                            \"job_title\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Job title\"\n",
    "                            },\n",
    "                            \"start_date\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Start date (MM/YY)\"\n",
    "                            },\n",
    "                            \"end_date\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"End date (MM/YY) or 'Present'\"\n",
    "                            },\n",
    "                            \"responsibilities\": {\n",
    "                                \"type\": \"array\",\n",
    "                                \"items\": {\n",
    "                                    \"type\": \"string\"\n",
    "                                },\n",
    "                                \"description\": \"Key responsibilities in bullet points\"\n",
    "                            },\n",
    "                            \"technologies\": {\n",
    "                                \"type\": \"array\",\n",
    "                                \"items\": {\n",
    "                                    \"type\": \"string\"\n",
    "                                },\n",
    "                                \"description\": \"List of relevant technologies used\"\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\n",
    "                \"experience\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "39df6d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIENCE_SP = \"You are an AI that reformats and structures job experience data from resumes into a JSON format.\"\n",
    "\n",
    "EXPERIENCE_HP = \"\"\"\n",
    "Using the following experience section in JSON format:\n",
    "{text_input}\n",
    "\n",
    "Reformat it into the following structured JSON format:\n",
    "{json_dump}\n",
    "\n",
    "Reformat the experience section into a structured JSON format, ensuring the following:\n",
    "- Clearly include fields for 'Company Name', 'Job Title', 'Dates of Employment', 'Responsibilities', and 'Technologies Used'.\n",
    "- Format job titles and dates as follows: 'Senior Consultant - Data Scientist (11/24 to Present)'.\n",
    "- Rewrite responsibilities into clear, action-based bullet points.\n",
    "- Include a 'Technology' field listing relevant technologies used.\n",
    "- Ensure consistency, readability, and completeness.\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8fef5144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the messages for the LLM\n",
    "text_input = pdf_text\n",
    "messages = [\n",
    "    SystemMessage(\n",
    "        content=EXPERIENCE_SP\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        content=(\n",
    "            EXPERIENCE_HP.format(\n",
    "                text_input=text_input,\n",
    "                json_dump=json.dumps(\n",
    "                    unsep_json[\"experience\"][\"json_schema\"], indent=2\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "    ),\n",
    "]\n",
    "\n",
    "response = llm.invoke(messages, functions=[unsep_json[\"experience\"][\"json_schema\"]])\n",
    "structured_data = response.additional_kwargs[\"function_call\"][\"arguments\"]\n",
    "json_structured_data = json.loads(structured_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ecd828",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "df046ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIENCE_SP2 = \"You are an AI that reformats and structures job experience data from resumes into a JSON format with clearly separated 'CGI Experience' and 'Other Experience' sections.\"\n",
    "\n",
    "EXPERIENCE_HP2 = \"\"\"\n",
    "Using the following experience section in JSON format:\n",
    "{text_input}\n",
    "\n",
    "Reformat it into the following structured JSON format, explicitly separating CGI Experience and Other Experience:\n",
    "\n",
    "Ensure the following:\n",
    "- Place a job under 'cgi_experience' only if the job clearly indicates that the work was performed at CGI or that the candidate was employed by CGI. This should be evident if the employer or client name explicitly includes 'CGI' (e.g., 'CGI', 'CGI Inc.', 'CGI Americas'). Do not classify a job as CGI Experience if the connection to CGI is merely tangential or if the job was performed for a CGI client without direct employment.\n",
    "** IMPORTANT - If no CGI-related jobs are present, leave the 'cgi_experience' section empty. **\n",
    "- All other jobs should be placed under 'other_experience'.\n",
    "\n",
    "For CGI experience format:\n",
    "- client_or_sector: Use client name if available (e.g., \"Bank of America\"), otherwise use sector (e.g., \"Financial Services\")\n",
    "- position_title (keep original job title)\n",
    "- dates (formatted as \"MM/YY to MM/YY\" or \"MM/YY to Present\")\n",
    "- responsibilities (as action-based bullet points)\n",
    "- technology (as an array of technologies used)\n",
    "\n",
    "For other experience format:\n",
    "- company (company name)\n",
    "- position_title (keep original job title)\n",
    "- dates (formatted as \"MM/YY to MM/YY\" or \"MM/YY to Present\")\n",
    "- responsibilities (as action-based bullet points)\n",
    "- technology (as an array of technologies used)\n",
    "\n",
    "Rewrite responsibilities into clear, action-based bullet points if needed.\n",
    "\"\"\"\n",
    "\n",
    "# Define the structured output schema\n",
    "structured_output_schema = {\n",
    "    \"name\": \"format_experience\",\n",
    "    \"description\": \"Formats and separates job experience data into CGI and Other experience sections\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"cgi_experience\": {\n",
    "                \"type\": \"array\",\n",
    "                \"description\": \"Experience specifically at CGI\",\n",
    "                \"items\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"client_or_sector\": {\"type\": \"string\", \"description\": \"Name of the client OR industry sector\"},\n",
    "                        \"position_title\": {\"type\": \"string\", \"description\": \"Job title\"},\n",
    "                        \"dates\": {\"type\": \"string\", \"description\": \"Employment dates\"},\n",
    "                        \"responsibilities\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "                        \"technology\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"other_experience\": {\n",
    "                \"type\": \"array\",\n",
    "                \"description\": \"Experience at companies other than CGI\",\n",
    "                \"items\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"company\": {\"type\": \"string\", \"description\": \"Company name\"},\n",
    "                        \"position_title\": {\"type\": \"string\", \"description\": \"Job title\"},\n",
    "                        \"dates\": {\"type\": \"string\", \"description\": \"Employment dates\"},\n",
    "                        \"responsibilities\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "                        \"technology\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"cgi_experience\", \"other_experience\"]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "44b431b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=EXPERIENCE_SP2),\n",
    "    HumanMessage(\n",
    "        content=(\n",
    "            EXPERIENCE_HP2.format(\n",
    "                text_input=json.dumps(json_structured_data, indent=2),\n",
    "                json_dump=json.dumps(\n",
    "                    structured_output_schema, indent=2\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "    ),\n",
    "]\n",
    "response = llm.invoke(messages, functions=[structured_output_schema])\n",
    "structured_data = response.additional_kwargs[\"function_call\"][\"arguments\"]\n",
    "json_structured_data = json.loads(structured_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b674ac79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cgi_experience': [],\n",
       " 'other_experience': [{'company': 'Amazon',\n",
       "   'position_title': 'Language Engineer I',\n",
       "   'dates': '11/25 to Present',\n",
       "   'responsibilities': ['Collaborated with scientists and engineers to design APIs, evaluate LLM performance, and develop scalable solutions for language data production and analysis.',\n",
       "    'Engineered prompts for generative AI, automated workflows, and performed data analysis using Python and scripting tools.',\n",
       "    'Managed customer-facing ML and deterministic models, resolved production issues, deployed Alexa language updates, and tested new features with modeling tools.'],\n",
       "   'technology': ['Python', 'Scripting tools', 'Generative AI']},\n",
       "  {'company': 'TEKsystems at Meta',\n",
       "   'position_title': 'Machine Learning Engineer',\n",
       "   'dates': '02/24 to 09/24',\n",
       "   'responsibilities': ['Performed extensive feature replacement for ads-based models to ensure privacy compliance while retaining neutral metrics.',\n",
       "    'Implemented innovative feature-replacement algorithms on models of varying architectures in a consolidated feature space.',\n",
       "    'Conducted thorough testing and analysis through offline experimentation and online A/B testing.',\n",
       "    'Collaborated closely with cross-functional teams to facilitate a seamless transition to privacy-compliant models.'],\n",
       "   'technology': ['Python', 'Feature-replacement algorithms']},\n",
       "  {'company': 'TEKsystems at Meta',\n",
       "   'position_title': 'Prompt Engineering Team Lead',\n",
       "   'dates': '07/23 to 02/24',\n",
       "   'responsibilities': ['Led a comprehensive assessment of conversational abilities of generative AI models.',\n",
       "    'Established a novel human evaluation process to measure specific conversational criteria, developing metrics for safety, accuracy, naturalness, and engagement levels of AI responses.',\n",
       "    'Constructed effective prompt-based finetuning strategies, improved through adversarial and red-teaming experimentation.',\n",
       "    'Developed a backend Pythonic system to augment human evaluation.',\n",
       "    'Conducted strategic evaluations of team capabilities to allocate daily tasks, improving efficiency, reducing learning curves, and optimizing resource allocation.'],\n",
       "   'technology': ['Python', 'Generative AI', 'Backend systems']},\n",
       "  {'company': 'Geering Up STEM Outreach UBC',\n",
       "   'position_title': 'Quantum Computing Curriculum Developer',\n",
       "   'dates': '05/23 to 08/23',\n",
       "   'responsibilities': ['Collaborated with industry professionals and graduate students to develop age-appropriate curriculum materials for high school workshops, camps, and teacher training programs.',\n",
       "    'Created engaging and interactive learning experiences.',\n",
       "    'Presented curriculum to instructors, provided training sessions, and guided them in the delivery of quantum computing workshops and camps.',\n",
       "    'Collected and analyzed participant feedback to continuously improve curriculum content and instructional methods.'],\n",
       "   'technology': ['Python', 'Curriculum development']},\n",
       "  {'company': 'Infosys',\n",
       "   'position_title': 'Network Systems Engineer',\n",
       "   'dates': '07/21 to 07/22',\n",
       "   'responsibilities': ['Trained in Networking, Unix, Routing, Switching, Network Automation with Ansible, and Agile and Scrum practices.',\n",
       "    'Implemented design, documentation, and project management in EDGE DCs for Daimler as a part of the Everest Project.'],\n",
       "   'technology': ['Networking', 'Unix', 'Ansible', 'Agile']}]}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_structured_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e41fac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b9192e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERAL_EXPERIENCE_SP = \"You are an AI that reformats and structures job experience data from resumes into a JSON format.\"\n",
    "\n",
    "GENERAL_EXPERIENCE_HP = \"\"\"\n",
    "Using the following experience section in JSON format:\n",
    "{text_input}\n",
    "\n",
    "Reformat it into the following structured JSON format:\n",
    "{json_dump}\n",
    "\n",
    "Reformat the experience section into a structured JSON format, ensuring the following:\n",
    "- Clearly include fields for 'Company Name', 'Job Title', 'Dates of Employment', 'Responsibilities', and 'Technologies Used'.\n",
    "- Format job titles and dates as follows: 'Senior Consultant - Data Scientist (11/24 to Present)'.\n",
    "- Rewrite responsibilities into clear, action-based bullet points.\n",
    "- Include a 'Technology' field listing relevant technologies used.\n",
    "- Ensure consistency, readability, and completeness.\n",
    "\"\"\"\n",
    "\n",
    "SEP_EXPERIENCE_SP = \"You are an AI that reformats and structures job experience data from resumes into a JSON format with clearly separated 'CGI Experience' and 'Other Experience' sections.\"\n",
    "\n",
    "SEP_EXPERIENCE_HP2 = \"\"\"\n",
    "Using the following experience section in JSON format:\n",
    "{text_input}\n",
    "\n",
    "Reformat it into the following structured JSON format, explicitly separating CGI Experience and Other Experience:\n",
    "\n",
    "Ensure the following:\n",
    "- Place a job under 'cgi_experience' only if the job clearly indicates that the work was performed at CGI or that the candidate was employed by CGI. This should be evident if the employer or client name explicitly includes 'CGI' (e.g., 'CGI', 'CGI Inc.', 'CGI Americas'). Do not classify a job as CGI Experience if the connection to CGI is merely tangential or if the job was performed for a CGI client without direct employment.\n",
    "** IMPORTANT - If no CGI-related jobs are present, leave the 'cgi_experience' section empty. **\n",
    "- All other jobs should be placed under 'other_experience'.\n",
    "\n",
    "For CGI experience format:\n",
    "- client_or_sector: Use client name if available (e.g., \"Bank of America\"), otherwise use sector (e.g., \"Financial Services\")\n",
    "- position_title (keep original job title)\n",
    "- dates (formatted as \"MM/YY to MM/YY\" or \"MM/YY to Present\")\n",
    "- responsibilities (as action-based bullet points)\n",
    "- technology (as an array of technologies used)\n",
    "\n",
    "For other experience format:\n",
    "- company (company name)\n",
    "- position_title (keep original job title)\n",
    "- dates (formatted as \"MM/YY to MM/YY\" or \"MM/YY to Present\")\n",
    "- responsibilities (as action-based bullet points)\n",
    "- technology (as an array of technologies used)\n",
    "\n",
    "Rewrite responsibilities into clear, action-based bullet points if needed.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1211ed3a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'general_experience_schema' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     25\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m chain\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Create the chains\u001b[39;00m\n\u001b[32m     28\u001b[39m general_experience_chain = create_function_chain(\n\u001b[32m     29\u001b[39m     general_experience_prompt, \n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     \u001b[43mgeneral_experience_schema\u001b[49m\n\u001b[32m     31\u001b[39m )\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# For the second chain, we need to format the input JSON properly\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mformat_for_second_chain\u001b[39m(data):\n",
      "\u001b[31mNameError\u001b[39m: name 'general_experience_schema' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "# Define your prompts\n",
    "general_experience_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", GENERAL_EXPERIENCE_SP),\n",
    "    (\"human\", GENERAL_EXPERIENCE_HP)\n",
    "])\n",
    "\n",
    "separated_experience_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", SEP_EXPERIENCE_SP),\n",
    "    (\"human\", SEP_EXPERIENCE_HP2)\n",
    "])\n",
    "\n",
    "# Define your chains with function calling\n",
    "def create_function_chain(prompt_template, function_schema):\n",
    "    openai_function = convert_to_openai_function(function_schema)\n",
    "    chain = prompt_template | llm.bind(\n",
    "        functions=[openai_function], \n",
    "        function_call={\"name\": function_schema[\"name\"]}\n",
    "    )\n",
    "    return chain\n",
    "\n",
    "# Create the chains\n",
    "general_experience_chain = create_function_chain(\n",
    "    general_experience_prompt, \n",
    "    general_experience_schema\n",
    ")\n",
    "\n",
    "# For the second chain, we need to format the input JSON properly\n",
    "def format_for_second_chain(data):\n",
    "    if isinstance(data, str):\n",
    "        # If the output is a string (JSON), parse it\n",
    "        return {\"text_input\": data}\n",
    "    elif isinstance(data, dict) and \"function_call\" in data.additional_kwargs:\n",
    "        # If it's already a response with function call\n",
    "        args = data.additional_kwargs[\"function_call\"][\"arguments\"]\n",
    "        return {\"text_input\": args}\n",
    "    else:\n",
    "        # Otherwise, just convert to JSON string\n",
    "        return {\"text_input\": json.dumps(data, indent=2)}\n",
    "\n",
    "separated_experience_chain = create_function_chain(\n",
    "    separated_experience_prompt,\n",
    "    separated_experience_schema\n",
    ")\n",
    "\n",
    "# Build the pipeline using the | operator\n",
    "pipeline = (\n",
    "    general_experience_chain \n",
    "    | (lambda x: json.loads(x.additional_kwargs[\"function_call\"][\"arguments\"])) \n",
    "    | (lambda x: {\"text_input\": json.dumps(x, indent=2)})\n",
    "    | separated_experience_chain\n",
    "    | (lambda x: json.loads(x.additional_kwargs[\"function_call\"][\"arguments\"]))\n",
    ")\n",
    "\n",
    "# Example usage with proper input formatting\n",
    "def process_resume(pdf_text):\n",
    "    return pipeline.invoke({\"text_input\": pdf_text, \"json_dump\": json.dumps(general_experience_schema, indent=2)})\n",
    "\n",
    "# Run the pipeline\n",
    "result = process_resume(pdf_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6411b45c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"cgi_experience\": [],\n",
      "  \"other_experience\": [\n",
      "    {\n",
      "      \"company\": \"Amazon\",\n",
      "      \"position_title\": \"Language Engineer I\",\n",
      "      \"dates\": \"11/25 to Present\",\n",
      "      \"responsibilities\": [\n",
      "        \"Collaborated with scientists and engineers to design APIs, evaluate LLM performance, and develop scalable solutions for language data production and analysis.\",\n",
      "        \"Engineered prompts for generative AI, automated workflows, and performed data analysis using Python and scripting tools.\",\n",
      "        \"Managed customer-facing ML and deterministic models, resolved production issues, deployed Alexa language updates, and tested new features with modeling tools.\"\n",
      "      ],\n",
      "      \"technology\": [\"Python\", \"Scripting Tools\"]\n",
      "    },\n",
      "    {\n",
      "      \"company\": \"TEKsystems at Meta\",\n",
      "      \"position_title\": \"Machine Learning Engineer\",\n",
      "      \"dates\": \"02/24 to 09/24\",\n",
      "      \"responsibilities\": [\n",
      "        \"Performed extensive feature replacement for ads-based models to ensure privacy compliance while retaining neutral metrics.\",\n",
      "        \"Implemented innovative feature-replacement algorithms on models of varying architectures in a consolidated feature space.\",\n",
      "        \"Conducted thorough testing and analysis through offline experimentation and online A/B testing.\",\n",
      "        \"Collaborated closely with cross-functional teams to facilitate a seamless transition to privacy-compliant models.\"\n",
      "      ],\n",
      "      \"technology\": [\"Python\", \"Machine Learning\"]\n",
      "    },\n",
      "    {\n",
      "      \"company\": \"TEKsystems at Meta\",\n",
      "      \"position_title\": \"Prompt Engineering Team Lead\",\n",
      "      \"dates\": \"07/23 to 02/24\",\n",
      "      \"responsibilities\": [\n",
      "        \"Led a comprehensive assessment of conversational abilities of generative AI models.\",\n",
      "        \"Established a novel human evaluation process to measure specific conversational criteria, developed metrics for safety, accuracy, naturalness, and engagement levels of AI responses.\",\n",
      "        \"Constructed effective prompt-based fine-tuning strategies, improved through adversarial and red-teaming experimentation.\",\n",
      "        \"Developed a backend Pythonic system to augment human evaluation.\",\n",
      "        \"Conducted strategic evaluations of team capabilities to allocate daily tasks, improving efficiency, reducing learning curves, and optimizing resource allocation.\"\n",
      "      ],\n",
      "      \"technology\": [\"Python\", \"Generative AI\", \"Prompt Engineering\"]\n",
      "    },\n",
      "    {\n",
      "      \"company\": \"Geering Up STEM Outreach UBC\",\n",
      "      \"position_title\": \"Quantum Computing Curriculum Developer\",\n",
      "      \"dates\": \"05/23 to 08/23\",\n",
      "      \"responsibilities\": [\n",
      "        \"Collaborated with industry professionals and graduate students to develop age-appropriate curriculum materials for high school workshops, camps, and teacher training programs.\",\n",
      "        \"Created engaging and interactive learning experiences.\",\n",
      "        \"Presented curriculum to instructors, provided training sessions, and guided them in the delivery of quantum computing workshops and camps.\",\n",
      "        \"Collected and analyzed participant feedback to continuously improve curriculum content and instructional methods, aiming to enhance the educational experience.\"\n",
      "      ],\n",
      "      \"technology\": []\n",
      "    },\n",
      "    {\n",
      "      \"company\": \"Infosys\",\n",
      "      \"position_title\": \"Network Systems Engineer\",\n",
      "      \"dates\": \"07/21 to 07/22\",\n",
      "      \"responsibilities\": [\n",
      "        \"Trained in Networking, Unix, Routing, Switching, Network Automation with Ansible, and Agile and Scrum practices.\",\n",
      "        \"Implemented design, documentation, and project management in EDGE DCs for Daimler as part of the Everest Project.\"\n",
      "      ],\n",
      "      \"technology\": [\"Unix\", \"Ansible\", \"Networking\"]\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# with langchain:\n",
    "\n",
    "from langchain.chains import SequentialChain\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "# First prompt template for extracting experience\n",
    "experience_extraction_prompt = PromptTemplate(\n",
    "    input_variables=[\"text_input\"],\n",
    "    template=\"\"\"\n",
    "    Extract all job experience from the following resume text:\n",
    "    {text_input}\n",
    "    \n",
    "    Format each job with company, job title, dates, responsibilities, and technologies used.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Second prompt template for separating CGI and other experience\n",
    "experience_formatting_prompt = PromptTemplate(\n",
    "    input_variables=[\"text_input\"],\n",
    "    template=\"\"\"\n",
    "    Using the following experience section in JSON format:\n",
    "    {text_input}\n",
    "    \n",
    "    Reformat it into structured JSON format, explicitly separating CGI Experience and Other Experience:\n",
    "    \n",
    "    Ensure the following:\n",
    "    - Place a job under 'cgi_experience' only if the job clearly indicates that the work was performed at CGI or that the candidate was employed by CGI. This should be evident if the employer or client name explicitly includes 'CGI'.\n",
    "    - If no CGI-related jobs are present, leave the 'cgi_experience' section empty.\n",
    "    - All other jobs should be placed under 'other_experience'.\n",
    "    \n",
    "    For CGI experience format:\n",
    "    - client_or_sector: Use client name if available, otherwise use sector\n",
    "    - position_title (keep original job title)\n",
    "    - dates (formatted as \"MM/YY to MM/YY\" or \"MM/YY to Present\")\n",
    "    - responsibilities (as action-based bullet points)\n",
    "    - technology (as an array of technologies used)\n",
    "    \n",
    "    For other experience format:\n",
    "    - company (company name)\n",
    "    - position_title (keep original job title)\n",
    "    - dates (formatted as \"MM/YY to MM/YY\" or \"MM/YY to Present\")\n",
    "    - responsibilities (as action-based bullet points)\n",
    "    - technology (as an array of technologies used)\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Create the chains\n",
    "extract_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=experience_extraction_prompt,\n",
    "    output_key=\"extracted_experience\"\n",
    ")\n",
    "\n",
    "format_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=experience_formatting_prompt,\n",
    "    output_key=\"formatted_experience\"\n",
    ")\n",
    "\n",
    "# Combine into sequential chain\n",
    "sequential_chain = SequentialChain(\n",
    "    chains=[extract_chain, format_chain],\n",
    "    input_variables=[\"text_input\"],\n",
    "    output_variables=[\"formatted_experience\"]\n",
    ")\n",
    "\n",
    "# Use the chain\n",
    "result = sequential_chain({\"text_input\": pdf_text})\n",
    "print(result[\"formatted_experience\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dba83f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d2bc43bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# json_structured_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97904001",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b6a3da82-b039-4122-8b9d-99286936e15d",
   "metadata": {},
   "source": [
    "### Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5181bc1f-41f7-44d2-94be-18f12633b4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Professional Summary\n",
    "messages = [\n",
    "    SystemMessage(\n",
    "        content=\"You are an AI that takes structured resumes in JSON format and writes a compelling, professional summary of the applicant.\"\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        content=(\n",
    "            \"Using the following structured resume data in JSON format:\\n\\n\"\n",
    "            f\"{structured_data}\\n\\n\"\n",
    "            \"Write a well-crafted, three-paragraph professional profile of the applicant in the third person. \"\n",
    "            \"Keep a good balance of detailed and concise. Do not use AI-isms\"\n",
    "            \"Incorporate their professional summary, work experience, education, skills, certifications, and any notable achievements. \"\n",
    "            \"Highlight their expertise, impact, and technical skills, ensuring the profile flows naturally and is engaging.\"\n",
    "        )\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "response = llm.invoke(messages, functions=[json_schema])\n",
    "\n",
    "profile = response.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f117f118-487b-477d-af93-19aade79a609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Years of Experience\n",
    "messages = [\n",
    "    SystemMessage(\n",
    "        content=\"You are an AI that takes in a professional summary and determines the applicants years of experience.\"\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        content=(\n",
    "            \"Using the following professional summary:\\n\\n\"\n",
    "            f\"{profile}\\n\\n\"\n",
    "            \"Write a very concise header desribing their experience in the following format:\\n <X> years experience in <X_category>\\nex: 5 years of experience in Software Development\"\n",
    "        )\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "# Call the LLM with function calling enabled\n",
    "response = llm.invoke(messages, functions=[json_schema])\n",
    "\n",
    "years_exp = response.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "586aa8d0-e440-46f1-9453-fc20a722e677",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm(overall, section, text_input=pdf_text):\n",
    "    messages = [\n",
    "        SystemMessage(\n",
    "            content=overall[section]['system_prompt']\n",
    "        ),\n",
    "        HumanMessage(\n",
    "            content=(overall[section]['human_prompt'].format(text_input=text_input, json_dump=json.dumps(overall[section]['json_schema'], indent=2)))\n",
    "        ), \n",
    "    ]\n",
    "    response = llm.invoke(messages, functions=[overall[section]['json_schema']])\n",
    "    structured_data = response.additional_kwargs[\"function_call\"][\"arguments\"]\n",
    "    json_structured_data = json.loads(structured_data)\n",
    "    \n",
    "    return json_structured_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "08c3fc43-9f37-43d7-8158-0f6151132bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"overall.json\", 'r') as file:\n",
    "    overall = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8ce47984-1269-41c5-abdc-b1505386052f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading...\n",
      "\t>> Completed key: experience\n",
      "\t>> Completed key: volunteer\n",
      "\t>> Completed key: other_sections\n",
      "\t>> Completed key: skills_summary\n"
     ]
    }
   ],
   "source": [
    "res_dict = dict()\n",
    "\n",
    "print(\"Loading...\")\n",
    "for key in overall.keys():\n",
    "    res_dict[key] = call_llm(overall, key, pdf_text)\n",
    "    print(f\"\\t>> Completed key: {key}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2b86ec-4cb3-4787-be55-2aea71e67e0c",
   "metadata": {},
   "source": [
    "# Formatting to doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae45d1e5-6305-4da2-a484-e115a22b7253",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replicate_section(doc, start_tag, end_tag, replacements, times_to_repeat):\n",
    "    \"\"\"\n",
    "    Duplicates the section between start_tag and end_tag (inclusive), replacing the tags with specified values.\n",
    "    \"\"\"\n",
    "   \n",
    "    para_group = []\n",
    "    inside_section = False\n",
    "    end_para = None\n",
    "    \n",
    "    for paragraph in doc.paragraphs:\n",
    "        if start_tag in paragraph.text:\n",
    "            inside_section = True\n",
    "            paragraph.text = paragraph.text.replace(f\"{start_tag}\", \"\")\n",
    "        if inside_section:\n",
    "            para_group.append(paragraph)\n",
    "        if end_tag in paragraph.text and inside_section:\n",
    "            end_para = paragraph\n",
    "            inside_section = False\n",
    "            paragraph.text = paragraph.text.replace(f\"{end_tag}\", \"\")\n",
    "            break \n",
    "    \n",
    "    end_para = end_para._p\n",
    "    for i in range(times_to_repeat):\n",
    "        new_para_lst = []\n",
    "        for paragraph in para_group:\n",
    "            paragraph.text = paragraph.text.replace(f\"{start_tag}\", \"\")\n",
    "            paragraph.text = paragraph.text.replace(f\"{end_tag}\", \"\")\n",
    "            \n",
    "            new_paragraph = doc.add_paragraph()\n",
    "            new_paragraph.alignment = paragraph.alignment\n",
    "            new_paragraph.style = paragraph.style\n",
    "            \n",
    "\n",
    "            new_paragraph.text = paragraph.text    \n",
    "            new_para_lst.append(new_paragraph)\n",
    "            \n",
    "        for para in new_para_lst:\n",
    "            end_para.addnext(para._p)\n",
    "            end_para = para._p\n",
    "\n",
    "# def replicate_row(doc, key, res_dict_skills):\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def replace_text_in_docx(doc, replacements):\n",
    "    \"\"\"\n",
    "    Replaces text in a DOCX file, and replicates sections based on the specified start and end tags.\n",
    "    \"\"\"\n",
    "    for key, value in replacements:\n",
    "        for paragraph in doc.paragraphs:\n",
    "        \n",
    "            if key in paragraph.text:\n",
    "                if isinstance(value, list):\n",
    "                    paragraph.style = 'ListBullet'\n",
    "                    x_par = paragraph._p\n",
    "                    if len(value) > 1:\n",
    "                        paragraph.text = paragraph.text.replace(key, value[0])\n",
    "                        value = value[1:]\n",
    "                        for bp in value[::-1]:\n",
    "                            para = doc.add_paragraph(bp, style='ListBullet')\n",
    "                            x_par.addnext(para._p)\n",
    "                else:\n",
    "                    paragraph.text = paragraph.text.replace(key, value)\n",
    "                break\n",
    "                \n",
    "        \n",
    "        for table in doc.tables:\n",
    "            for row in table.rows:\n",
    "                for cell in row.cells:\n",
    "                    for paragraph in cell.paragraphs:\n",
    "                        if key in paragraph.text:\n",
    "                            if isinstance(value, list):\n",
    "                                paragraph.style = 'ListBullet'\n",
    "                                x_par = paragraph._p\n",
    "                                paragraph.text = paragraph.text.replace(key, value[0])\n",
    "                                value = value[1:]\n",
    "                                for bp in value:\n",
    "                                    para = doc.add_paragraph(bp, style='ListBullet')\n",
    "                                    x_par.addnext(para._p)\n",
    "                            else:\n",
    "                                paragraph.text = paragraph.text.replace(key, value)\n",
    "                            break\n",
    "                        \n",
    "\n",
    "def handle_skills_summary(doc, replacements, res_dict_skills):\n",
    "    for key, value in replacements:\n",
    "        for table in doc.tables:\n",
    "            for row in table.rows:\n",
    "                if any(key in para.text for cell in row.cells for para in cell.paragraphs):\n",
    "                    cleaned_key = key.replace(\"{\", \"\").replace(\"}\", \"\")\n",
    "                    times_to_repeat = len(res_dict_skills[cleaned_key]) - 1\n",
    "                    for _ in range(times_to_repeat):\n",
    "                        new_row = table.add_row()\n",
    "                        for i, new_cell in enumerate(new_row.cells):\n",
    "                            old_para = row.cells[i].paragraphs[0]\n",
    "                            new_cell.text = old_para.text\n",
    "                            new_cell.paragraphs[0].style = old_para.style\n",
    "                        row._tr.addnext(new_row._tr)\n",
    "\n",
    "\n",
    "def replace_text_in_table(doc, replacements, res_dict):\n",
    "    \"\"\"\n",
    "    Replaces text in a DOCX file, and replicates sections based on the specified start and end tags.\n",
    "    \"\"\"\n",
    "    for key, value in replacements:\n",
    "        for table in doc.tables:\n",
    "            for row in table.rows:\n",
    "                for cell in row.cells:\n",
    "                    replacement_made = False\n",
    "                    for paragraph in cell.paragraphs:\n",
    "                        if key in paragraph.text:\n",
    "                            paragraph.text = paragraph.text.replace(key, value)\n",
    "                            replacement_made = True\n",
    "                            break\n",
    "                    if replacement_made:\n",
    "                        break  \n",
    "                if replacement_made:\n",
    "                    break \n",
    "            if replacement_made:\n",
    "                break  \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "966b87c7-5f9b-4ddc-8f9d-6001ec4d66d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'contact': {'name': 'ANANYA APPARAJU',\n",
       "  'email': 'ananya.apparaju@gmail.com',\n",
       "  'phone': '',\n",
       "  'location': '',\n",
       "  'linkedin': '',\n",
       "  'website': ''},\n",
       " 'professional_summary': 'Passionate about solving complex problems in natural language processing and information retrieval. Skilled in machine learning and deep learning techniques. Experienced in Python, Java, and C++. Committed to continuous learning and driving innovation in language technology.',\n",
       " 'education': [{'degree': 'Master of Data Science',\n",
       "   'field_of_study': 'Computational Linguistics',\n",
       "   'institution': 'University of British Columbia',\n",
       "   'location': 'Vancouver, BC',\n",
       "   'graduation_year': '06/2023'},\n",
       "  {'degree': 'Bachelor of Technology',\n",
       "   'field_of_study': 'Electronics and Communications Engineering',\n",
       "   'institution': 'Jawaharlal Nehru Technological University',\n",
       "   'location': 'Hyderabad, Telangana, India',\n",
       "   'graduation_year': '11/2020'}],\n",
       " 'experience': [{'job_title': 'Language Engineer I',\n",
       "   'company': 'Amazon',\n",
       "   'location': 'Chennai, India',\n",
       "   'start_date': '11/2025',\n",
       "   'end_date': 'Present',\n",
       "   'responsibilities': 'Collaborated with scientists and engineers to design APIs, evaluate LLM performance, and develop scalable solutions for language data production and analysis. Engineered prompts for generative AI, automated workflows, and performed data analysis using Python and scripting tools. Managed customer-facing ML and deterministic models, resolved production issues, deployed Alexa language updates, and tested new features with modeling tools.'},\n",
       "  {'job_title': 'Machine Learning Engineer',\n",
       "   'company': 'TEKsystems at Meta',\n",
       "   'location': 'Vancouver, BC',\n",
       "   'start_date': '02/2024',\n",
       "   'end_date': '09/2024',\n",
       "   'responsibilities': 'Performed extensive feature replacement for ads-based models to ensure privacy compliance while retaining neutral metrics. Implemented innovative feature-replacement algorithms on models of varying architectures in a consolidated feature space. Conducted thorough testing and analysis through offline experimentation and online A/B testing. Collaborated closely with cross-functional teams to facilitate a seamless transition to privacy-compliant models.'},\n",
       "  {'job_title': 'Prompt Engineering Team Lead',\n",
       "   'company': 'TEKsystems at Meta',\n",
       "   'location': 'Vancouver, BC',\n",
       "   'start_date': '07/2023',\n",
       "   'end_date': '02/2024',\n",
       "   'responsibilities': 'Led a comprehensive assessment of conversational abilities of generative AI models. Established a novel human evaluation process to measure specific conversational criteria. Developed metrics for safety, accuracy, naturalness, and engagement levels of AI responses. Constructed effective prompt-based finetuning strategies, improved thorough adversarial and red-teaming experimentation. Developed a backend Pythonic system to augment human evaluation. Conducted strategic evaluations of team capabilities to allocate daily tasks which improved efficiency, reduced learning curves, and optimized resource allocation.'},\n",
       "  {'job_title': 'Quantum Computing Curriculum Developer',\n",
       "   'company': 'Geering Up STEM Outreach UBC',\n",
       "   'location': 'Vancouver, BC',\n",
       "   'start_date': '05/2023',\n",
       "   'end_date': '08/2023',\n",
       "   'responsibilities': 'Collaborated with industry professionals and graduate students to develop age-appropriate curriculum materials for high school workshops, camps, and teacher training programs, created engaging and interactive learning experiences. Presented curriculum to instructors, provided training sessions, and guided them in the delivery of quantum computing workshops and camps. Collected and analyzed participant feedback to continuously improve curriculum content and instructional methods, aiming to enhance the educational experience.'},\n",
       "  {'job_title': 'Network Systems Engineer',\n",
       "   'company': 'Infosys',\n",
       "   'location': 'Hyderabad, Telangana, India',\n",
       "   'start_date': '07/2021',\n",
       "   'end_date': '07/2022',\n",
       "   'responsibilities': 'Trained in Networking, Unix, Routing, Switching, Network Automation with Ansible, and Agile and Scrum practices. Implemented design, documentation, and project management in EDGE DCs for Daimler, as a part of the Everest Project.'}],\n",
       " 'skills': ['Python',\n",
       "  'SQL/NoSQL',\n",
       "  'Java',\n",
       "  'C/C++',\n",
       "  'Git',\n",
       "  'Unix/Ansible',\n",
       "  'HTML/CSS',\n",
       "  'React',\n",
       "  'Data Manipulation',\n",
       "  'Descriptive Statistics and Probability',\n",
       "  'Algorithms and Data Structures',\n",
       "  'Data Visualization',\n",
       "  'Statistical Inference',\n",
       "  'Supervised Learning',\n",
       "  'Regression',\n",
       "  'Computational Parsing',\n",
       "  'Computational Semantics',\n",
       "  'Unsupervised Learning',\n",
       "  'Computational Morphology',\n",
       "  'Machine Translation',\n",
       "  'Sentiment Analysis',\n",
       "  'NLP for Low Resource Languages',\n",
       "  'Prompt Engineering',\n",
       "  'Analog and Digital Networks',\n",
       "  'Switching and Logic Theory',\n",
       "  'Digital Signal and Image Processing'],\n",
       " 'volunteer_experience': [{'organization': 'Code in Place, offered online by Stanford University',\n",
       "   'role': 'Section Leader',\n",
       "   'location': '',\n",
       "   'start_date': '',\n",
       "   'end_date': '',\n",
       "   'description': \"Facilitated weekly discussion sessions for 8-10 students as a volunteer section leader to supplement professors' lectures in the 6-week introductory Python programming course. Collaborated with a diverse group of 700 volunteer teachers and 9000 students from around the world to deliver a quality educational experience.\"}],\n",
       " 'projects': [{'title': 'NLP-Based Analysis of Ontario Housing Legislation and Judicial Relevance Factors in Decisions on Eviction (Capstone Project)',\n",
       "   'description': 'Conducted text mining and comprehensive analysis of 44,228 Ontario Housing cases using NLP principles. Developed a validated model to determine recurring factors that judges consider when ordering or delaying evictions. Designed and built an API/dashboard to provide quasi-real-time outcomes by applying the model to any Residential Tenancy Board (RTB) decisions.',\n",
       "   'technologies': [],\n",
       "   'link': ''},\n",
       "  {'title': 'SIGMORPHON Shared Task on Interlinear Glossing',\n",
       "   'description': \"Published a paper that was accepted into ACL titled 'Glossy Bytes- Neural Glossing using Subword Encoding.' Researched ways to automate the creation of interlinear glossed text, a key annotated data type in linguistic fieldwork and often the sole form of annotated data accessible for NLP work in low-resource languages. Developed a transformer-based deep learning model that produces grammatical descriptions at the morpheme level based on input sentences adhering to the Leipzig glossing conventions.\",\n",
       "   'technologies': [],\n",
       "   'link': ''},\n",
       "  {'title': 'Brainy: The Mental Health Screening Chatbot',\n",
       "   'description': 'Finetuned an LLM using cutting edge prompt engineering techniques to administer psychological screening surveys in a conversational manner. Developed a web app interface for the screening tests.',\n",
       "   'technologies': [],\n",
       "   'link': ''},\n",
       "  {'title': 'The Bookdel Test: Examining Female Participation in Jane Austen Novels',\n",
       "   'description': \"Adapted the Bechdel Test for classic literature using advanced corpus linguistics techniques using Jane Austen novels as a baseline. Annotated a collection of Jane Austens bibliography to assess gender representation. Developed a searchable web app for the entire annotated corpora. Featured on the UBC website's Data Science in Action tab.\",\n",
       "   'technologies': [],\n",
       "   'link': ''}],\n",
       " 'languages': [],\n",
       " 'awards': []}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structured_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "98bab668-1c01-4497-88dc-4d68cc53f84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_data[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9fd22d8a-402c-4984-9e45-0c44044de5dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lean Six Sigma Green Belt, Six Sigma Global Institute',\n",
       " 'Certified SAFe 6 Practitioner, Scaled Agile Inc.',\n",
       " 'SAP Student Recognition Award, SAP University Alliances, Dalhousie University',\n",
       " 'Lean Six Sigma Yellow Belt, Government of Nova Scotia']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ed_list\n",
    "certs = [f\"{i['name']}, {i['issuing_organization']}\" for i in structured_data['certifications']]\n",
    "certs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a559f592-888c-45c6-907d-4f75ddb7029a",
   "metadata": {},
   "outputs": [
    {
     "ename": "PackageNotFoundError",
     "evalue": "Package not found at 'resume_sample.docx'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPackageNotFoundError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 74\u001b[39m\n\u001b[32m     72\u001b[39m     doc.save(output_filename)\n\u001b[32m     73\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUpdated document saved as: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m \u001b[43mgenerate_resume\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mgenerate_resume\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      9\u001b[39m input_filename = \u001b[33m\"\u001b[39m\u001b[33mresume_sample.docx\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     10\u001b[39m output_filename = \u001b[33m\"\u001b[39m\u001b[33mupdated_resume.docx\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m doc = \u001b[43mDocument\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_filename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m full_name = structured_data[\u001b[33m'\u001b[39m\u001b[33mcontact\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mname\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     14\u001b[39m cgi_title = \u001b[33m\"\u001b[39m\u001b[33mConsultant\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jata.maccabe\\Documents\\GitHub\\cgi_resume_poc\\rg-venv\\Lib\\site-packages\\docx\\api.py:27\u001b[39m, in \u001b[36mDocument\u001b[39m\u001b[34m(docx)\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Return a |Document| object loaded from `docx`, where `docx` can be either a path\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[33;03mto a ``.docx`` file (a string) or a file-like object.\u001b[39;00m\n\u001b[32m     22\u001b[39m \n\u001b[32m     23\u001b[39m \u001b[33;03mIf `docx` is missing or ``None``, the built-in default document \"template\" is\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[33;03mloaded.\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     26\u001b[39m docx = _default_docx_path() \u001b[38;5;28;01mif\u001b[39;00m docx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m docx\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m document_part = cast(\u001b[33m\"\u001b[39m\u001b[33mDocumentPart\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mPackage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocx\u001b[49m\u001b[43m)\u001b[49m.main_document_part)\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m document_part.content_type != CT.WML_DOCUMENT_MAIN:\n\u001b[32m     29\u001b[39m     tmpl = \u001b[33m\"\u001b[39m\u001b[33mfile \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m is not a Word file, content type is \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jata.maccabe\\Documents\\GitHub\\cgi_resume_poc\\rg-venv\\Lib\\site-packages\\docx\\opc\\package.py:127\u001b[39m, in \u001b[36mOpcPackage.open\u001b[39m\u001b[34m(cls, pkg_file)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mopen\u001b[39m(\u001b[38;5;28mcls\u001b[39m, pkg_file: \u001b[38;5;28mstr\u001b[39m | IO[\u001b[38;5;28mbytes\u001b[39m]) -> OpcPackage:\n\u001b[32m    126\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return an |OpcPackage| instance loaded with the contents of `pkg_file`.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m     pkg_reader = \u001b[43mPackageReader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpkg_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    128\u001b[39m     package = \u001b[38;5;28mcls\u001b[39m()\n\u001b[32m    129\u001b[39m     Unmarshaller.unmarshal(pkg_reader, package, PartFactory)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jata.maccabe\\Documents\\GitHub\\cgi_resume_poc\\rg-venv\\Lib\\site-packages\\docx\\opc\\pkgreader.py:22\u001b[39m, in \u001b[36mPackageReader.from_file\u001b[39m\u001b[34m(pkg_file)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_file\u001b[39m(pkg_file):\n\u001b[32m     21\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return a |PackageReader| instance loaded with contents of `pkg_file`.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     phys_reader = \u001b[43mPhysPkgReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpkg_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m     content_types = _ContentTypeMap.from_xml(phys_reader.content_types_xml)\n\u001b[32m     24\u001b[39m     pkg_srels = PackageReader._srels_for(phys_reader, PACKAGE_URI)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jata.maccabe\\Documents\\GitHub\\cgi_resume_poc\\rg-venv\\Lib\\site-packages\\docx\\opc\\phys_pkg.py:21\u001b[39m, in \u001b[36mPhysPkgReader.__new__\u001b[39m\u001b[34m(cls, pkg_file)\u001b[39m\n\u001b[32m     19\u001b[39m         reader_cls = _ZipPkgReader\n\u001b[32m     20\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m PackageNotFoundError(\u001b[33m\"\u001b[39m\u001b[33mPackage not found at \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m % pkg_file)\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# assume it's a stream and pass it to Zip reader to sort out\u001b[39;00m\n\u001b[32m     23\u001b[39m     reader_cls = _ZipPkgReader\n",
      "\u001b[31mPackageNotFoundError\u001b[39m: Package not found at 'resume_sample.docx'"
     ]
    }
   ],
   "source": [
    "from docx.oxml.ns import qn\n",
    "from docx.oxml import OxmlElement\n",
    "from docx.shared import Pt  # Import Pt from docx.shared\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "def generate_resume():\n",
    "    # Input/Output\n",
    "    input_filename = \"../data/resume_sample.docx\"\n",
    "    output_filename = \"POC_UPDATE.docx\"\n",
    "    doc = Document(input_filename)\n",
    "    \n",
    "    full_name = structured_data['contact']['name']\n",
    "    cgi_title = \"Consultant\"\n",
    "    sector = \"Health Services\"\n",
    "    replacements = [\n",
    "        (\"{full_name}\", full_name),\n",
    "        (\"{cgi_title}\", cgi_title),\n",
    "        (\"{years_exp}\", years_exp),\n",
    "        (\"{professional_profile}\", profile),\n",
    "        (\"{industry}\", res_dict['other_sections']['industry_experience']),\n",
    "        (\"{tech_specs}\", res_dict['other_sections']['technical_specializations']),\n",
    "        (\"{expertise}\", res_dict['other_sections']['areas_of_expertise']),\n",
    "        (\"{languages}\", res_dict['other_sections']['languages']),\n",
    "        (\"{environment}\", res_dict['other_sections']['environments']),\n",
    "        (\"{tools}\", res_dict['other_sections']['tools_and_software'])\n",
    "    ]\n",
    "    \n",
    "    if 'certifications' in structured_data:\n",
    "        certs = [f\"{i['name']}, {i['issuing_organization']}\" for i in structured_data['certifications']]\n",
    "        replacements[\"{certs}\"] = certs\n",
    "    \n",
    "    # CGI Experience\n",
    "    cgi_exp =  res_dict['experience']['cgi_experience']\n",
    "    for exp in cgi_exp:\n",
    "        exp = {k: v for k, v in exp.items() if k in ['sector', 'job_title', 'start_date', 'end_date', 'responsibilities']}\n",
    "        for key, value in exp.items():\n",
    "            replacements.append( (\"{\" + key + \"}\", value) )\n",
    "    times_to_repeat = len(cgi_exp) -1\n",
    "    \n",
    "    replicate_section(doc, \"{begin_cgi_exp}\", \"{end_cgi_exp}\", replacements, times_to_repeat)\n",
    "\n",
    "    # Other Experience\n",
    "    o_exp =  res_dict['experience']['other_experience']\n",
    "    for exp in o_exp:\n",
    "        exp = {k: v for k, v in exp.items() if k in ['company', 'job_title', 'start_date', 'end_date', 'responsibilities']}\n",
    "        for key, value in exp.items():\n",
    "            replacements.append( (\"{\" + key + \"}\", value) )\n",
    "    times_to_repeat = len(o_exp) -1\n",
    "    replicate_section(doc, \"{begin_other_exp}\", \"{end_other_exp}\", replacements, times_to_repeat)\n",
    "\n",
    "    # Skills summary\n",
    "    table_reps = []\n",
    "    for key, value in res_dict['skills_summary'].items():\n",
    "        table_reps.append( (\"{\" + key + \"}\", value) )\n",
    "    ed_list = [f\"{el['degree']}, {el['field_of_study']} - {el['institution']}\" for el in structured_data['education']]\n",
    "    replacements.append( (\"{education_entry}\", ed_list) )\n",
    "\n",
    "\n",
    "    replace_text_in_docx(doc, replacements)\n",
    "    handle_skills_summary(doc, table_reps, res_dict['skills_summary'])\n",
    "\n",
    "    replacements = []\n",
    "    for key, value in res_dict['skills_summary'].items():\n",
    "        replacements.extend( [ (\"{\" + key + \"}\", v['skill']) for v in value] )\n",
    "        replacements.extend( [ (\"{num_years}\", str(v['years_of_experience'])) for v in value] )\n",
    "        replacements.extend( [ (\"{skill_level}\", str(v['skill_level'])) for v in value] )\n",
    "\n",
    "    replace_text_in_table(doc, replacements,  res_dict['skills_summary'])\n",
    "\n",
    "    \n",
    "    doc.save(output_filename)\n",
    "    print(f\"Updated document saved as: {output_filename}\")\n",
    "generate_resume()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bae6425-7fe7-4b7c-96cd-d3f9666b7b6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rg-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
